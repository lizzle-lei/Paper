==========================
线性代数
==========================

1.听完课留下的问题
------------------------

**a. 什么是张量？**

按照我自己的理解，其是pytorch中描述标量，向量，矩阵的一种表达，tensor。
张量可以理解是一个多维数组，相当于numpy中的nparray

**b. shape为(3,2,4)分别表示什么意思？**

3表示层，2表示行，4表示列

2.知识点总结回归
------------------
**a. 按照axis进行求和**

按照哪一个axis求和就丢掉哪一个维度，按照axis对应的轴进行压缩

假设一个tensor的shape为[5,4]
axis = 0 求sum，得到长度为4的向量
axis = 1 求sum,得到长度为5的向量

假设一个tensor的shape为[2, 5, 4]
axis = 0 求sum,得到shape为[5,4]的矩阵
axis = 1 求sum,得到shape为[2,4]的矩阵
axis = 2 求sum,得到shape为[2,5]的矩阵

如果求sum的时候将keepdim设置为true,即keepdim = true，就相当于把那一维设置为1
即axis = 0 求sum,得到shape为[1,5,4]的矩阵

3.听课的收获
---------------
特征向量就是不被矩阵改变方向的向量

一个向量乘上一个矩阵就相当于发生一个空间上的扭曲

范数的概念指的是矩阵的长度

